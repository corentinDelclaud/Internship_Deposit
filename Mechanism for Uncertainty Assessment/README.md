# Mechanism for Uncertainty Assessment

## Overview

This module implements a comprehensive uncertainty assessment mechanism for RAG (Retrieval-Augmented Generation) systems using DeepEval. It provides advanced metrics to quantify the reliability and uncertainty of responses generated by AI systems.

## Features

### DeepEval Evaluation Metrics
- **Answer Relevancy** - Response relevance
- **Faithfulness** - Faithfulness to source context
- **Contextual Precision** - Contextual precision
- **Contextual Relevancy** - Contextual relevance
- **Contextual Recall** - Contextual recall

### Uncertainty Assessment
- **Confidence Quantification** - Confidence scores for each response
- **Hallucination Detection** - Identification of unreliable responses
- **Coherence Analysis** - Internal consistency verification

## Installation

```bash
pip install -r requirements.txt
```

### Main Dependencies
```bash
deepeval>=0.21.0
transformers>=4.30.0
torch>=2.0.0
bitsandbytes>=0.41.0
lmformatenforcer
pydantic
```

### Environment Variables Configuration

Create a `.env` file with:
```bash
OPENAI_API_KEY=your_openai_key
DEEPEVAL_API_KEY=your_deepeval_key
HF_TOKEN=your_huggingface_token
```

## Usage

### Basic Evaluation

```bash
python RAG_DeepEval.py --input_file data/test_cases.json --output_file results/evaluation.json
```

### Evaluation with Custom Model

```bash
python RAG_DeepEval.py --model_name mistralai/Mistral-7B-v0.1 --batch_size 4
```

### Complete Evaluation with All Metrics

```python
from deepeval import evaluate
from deepeval.test_case import LLMTestCase

# Create test cases
test_cases = [
    LLMTestCase(
        input="What is the capital of France?",
        actual_output="Paris is the capital of France.",
        expected_output="Paris",
        context=["France is a country in Europe. Paris is its capital city."]
    )
]

# Evaluate with all metrics
evaluate(test_cases, metrics=[
    AnswerRelevancyMetric(),
    FaithfulnessMetric(),
    ContextualPrecisionMetric(),
    ContextualRelevancyMetric(),
    ContextualRecallMetric()
])
```

## Project Structure

```
Mechanism for Uncertainty Assessment/
├── RAG_DeepEval.py              # Main evaluation script
├── Uncertainty_module/          # Custom uncertainty module
├── assets/                      # Assets and static data
├── .deepeval/                   # DeepEval configuration
├── .deepeval_telemtry.txt      # DeepEval telemetry
├── requirements.txt             # Python dependencies
├── .env                         # Environment variables
└── README.md                    # This documentation
```

## Custom Model Configuration

### CustomModel Class

The module uses a `CustomModel` class that inherits from `DeepEvalBaseLLM`:

```python
class CustomModel(DeepEvalBaseLLM):
    def __init__(self):
        self.model_id = "mistralai/Mistral-7B-v0.1"
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_id)
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_id,
            device_map="auto",
            quantization_config=BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_quant_type="nf4",
                bnb_4bit_compute_dtype=torch.bfloat16
            )
        )
```

### Memory Optimizations
- **4-bit Quantization** with BitsAndBytesConfig
- **Automatic device mapping** for multi-GPU
- **Format enforcement** with LMFormatEnforcer

## Detailed Metrics

### 1. Answer Relevancy Metric
Evaluates whether the answer is relevant to the asked question.
```python
answer_relevancy = AnswerRelevancyMetric(threshold=0.7)
```

### 2. Faithfulness Metric
Verifies if the answer is faithful to the provided context.
```python
faithfulness = FaithfulnessMetric(threshold=0.8)
```

### 3. Contextual Precision
Measures the precision of retrieved context.
```python
contextual_precision = ContextualPrecisionMetric(threshold=0.7)
```

### 4. Contextual Relevancy
Evaluates the relevance of context to the question.
```python
contextual_relevancy = ContextualRelevancyMetric(threshold=0.7)
```

### 5. Contextual Recall
Measures if all relevant context has been retrieved.
```python
contextual_recall = ContextualRecallMetric(threshold=0.7)
```

## Data Format

### Test Case Structure

```json
{
    "test_cases": [
        {
            "input": "Asked question",
            "actual_output": "System-generated response",
            "expected_output": "Expected response",
            "context": [
                "Context 1",
                "Context 2",
                "..."
            ],
            "retrieval_context": [
                "Retrieved document 1",
                "Retrieved document 2"
            ]
        }
    ]
}
```

### Evaluation Results

```json
{
    "overall_score": 0.85,
    "metrics": {
        "answer_relevancy": 0.87,
        "faithfulness": 0.92,
        "contextual_precision": 0.81,
        "contextual_relevancy": 0.84,
        "contextual_recall": 0.79
    },
    "detailed_results": [...]
}
```

## Advanced Configuration

### Model Parameters

```python
# Quantization configuration
quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_use_double_quant=True
)

# Generation configuration
generation_config = {
    "max_new_tokens": 512,
    "temperature": 0.7,
    "top_p": 0.9,
    "do_sample": True
}
```

### JSON Schemas with LMFormatEnforcer

```python
from pydantic import BaseModel

class ResponseSchema(BaseModel):
    answer: str
    confidence: float
    reasoning: str

# Apply schema
parser = JsonSchemaParser(ResponseSchema.model_json_schema())
prefix_function = build_transformers_prefix_allowed_tokens_fn(
    tokenizer, parser
)
```

## Custom Uncertainty Module

### Uncertainty_module/

This folder contains custom implementations for:
- **Entropy Calculation** - Information uncertainty measurement
- **Confidence Calibration** - Confidence score adjustment
- **Anomaly Detection** - Aberrant response identification
- **Metric Aggregation** - Multiple uncertainty measure combination

## Monitoring and Logging

### DeepEval Dashboard
- Web interface for result visualization
- Comparison between different models
- Performance tracking over time

### Detailed Logs
```python
import logging

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('uncertainty_assessment.log'),
        logging.StreamHandler()
    ]
)
```

## Performance Optimization

### GPU Recommendations
- **Minimum** : RTX 3080 (10GB VRAM)
- **Recommended** : RTX 4090 (24GB VRAM)
- **Optimal** : A100 (40GB VRAM)

### Batch Processing
```python
# Batch processing for efficiency
batch_size = 4
for i in range(0, len(test_cases), batch_size):
    batch = test_cases[i:i+batch_size]
    results = evaluate(batch, metrics=all_metrics)
```

## Utility Scripts

### Data Preparation
```python
def prepare_test_cases(data_file):
    """Converts raw data to DeepEval test cases"""
    # Conversion implementation
    pass

def aggregate_results(results):
    """Aggregates evaluation results"""
    # Global statistics calculation
    pass
```

## Troubleshooting

### Common Issues

1. **GPU Memory Errors**
   ```python
   # Solution: Reduce model size or use quantization
   load_in_8bit=True  # or load_in_4bit=True
   ```

2. **DeepEval API Errors**
   ```bash
   # Check API keys
   deepeval login
   ```

3. **JSON Format Issues**
   ```python
   # Validate schema before evaluation
   validator = JsonSchemaValidator(schema)
   ```

## Usage Examples

### Complete RAG System Evaluation

```python
from RAG_DeepEval import evaluate_rag_system

# Load data
test_data = load_test_data("evaluation_dataset.json")

# Configure metrics
metrics_config = {
    "answer_relevancy": {"threshold": 0.7},
    "faithfulness": {"threshold": 0.8},
    "contextual_precision": {"threshold": 0.7}
}

# Evaluation
results = evaluate_rag_system(
    test_data=test_data,
    model_name="mistralai/Mistral-7B-v0.1",
    metrics_config=metrics_config,
    output_file="evaluation_results.json"
)

print(f"Overall score: {results['overall_score']:.3f}")
```

## Contributing

To contribute to the uncertainty assessment module:
1. Fork the repository
2. Create a feature branch
3. Add new metrics or improvements
4. Test with varied datasets
5. Open a Pull Request

## References

- [DeepEval Documentation](https://docs.deepeval.com/)
- [Uncertainty Quantification in NLP](https://arxiv.org/abs/2006.15376)
- [RAG Evaluation Metrics](https://arxiv.org/abs/2401.15884)
